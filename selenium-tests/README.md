# ğŸ¢ Enterprise-Grade Selenium Test Automation Framework

> **IWSVA System Updates Verification - Production-Ready Edition**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![Selenium](https://img.shields.io/badge/Selenium-4.16.0-green.svg)](https://www.selenium.dev/)
[![Pytest](https://img.shields.io/badge/Pytest-7.4.3-orange.svg)](https://pytest.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Enterprise Features](#-enterprise-features)
- [Project Structure](#-project-structure)
- [Quick Start](#-quick-start)
- [Configuration](#-configuration)
- [Running Tests](#-running-tests)
- [Test Reports](#-test-reports)
- [Debugging](#-debugging)
- [Best Practices](#-best-practices)
- [CI/CD Integration](#-cicd-integration)

---

## ğŸ¯ Overview

This is a **production-ready Selenium test automation framework** designed for enterprise QA teams. It implements industry best practices for:

âœ… **Problem Analysis & Debugging**
- Multi-level logging (DEBUG/INFO/WARNING/ERROR)
- Automatic screenshot capture on test failure
- HTML source code preservation
- Browser console logs extraction
- Detailed exception stack traces
- Test step tracking and performance metrics

âœ… **Enterprise-Level Standards**
- Comprehensive documentation (Google Docstring style)
- Test case ID and traceability matrix
- Data-driven testing (DDT)
- Allure test reporting
- CI/CD ready configuration
- PEP8 code compliance
- Test isolation and cleanup mechanisms
- Parallel execution support

---

## ğŸš€ Enterprise Features

### 1. Advanced Debugging Capabilities

#### Automatic Failure Artifact Capture
When a test fails, the framework **automatically captures**:

```python
artifacts/
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ TC-SYS-001_20250109_143025.png      # Failure screenshot
â”‚   â””â”€â”€ TC-SYS-001_20250109_143025.html     # Page HTML source
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ TC-SYS-001_20250109_143025_browser.log  # Browser console logs
â”‚   â””â”€â”€ TC-SYS-001_20250109_143025_info.json    # Page state snapshot
â””â”€â”€ videos/ (optional)
    â””â”€â”€ TC-SYS-001_20250109_143025.mp4      # Test execution video
```

#### Multi-Level Logging

```python
# Example log output
2025-01-09 14:30:25 [INFO] [TestLogger] ======================================
2025-01-09 14:30:25 [INFO] [TestLogger] TEST STARTED: TC-SYS-001 - test_kernel_version
2025-01-09 14:30:25 [INFO] [TestLogger] Description: Verify kernel version display
2025-01-09 14:30:25 [INFO] [TestLogger] ======================================
2025-01-09 14:30:26 [INFO] [SystemUpdatePage] Step 1: Navigate to System Updates page
2025-01-09 14:30:27 [INFO] [SystemUpdatePage] âœ“ Switched to frame: right
2025-01-09 14:30:28 [INFO] [SystemUpdatePage] âœ“ Kernel version extracted: 5.14.0-427.24.1.el9_4.x86_64
2025-01-09 14:30:28 [INFO] [TestLogger] âœ“ PASS - Kernel version: 5.14.0-427.24.1.el9_4.x86_64
2025-01-09 14:30:28 [INFO] [TestLogger] ======================================
2025-01-09 14:30:28 [INFO] [TestLogger] TEST PASSED: test_kernel_version
2025-01-09 14:30:28 [INFO] [TestLogger] Duration: 3.25s
2025-01-09 14:30:28 [INFO] [TestLogger] ======================================
```

### 2. Enterprise Test Standards

#### Comprehensive Documentation

Every test case includes:
- **Test Case ID**: Unique identifier (e.g., TC-SYS-001)
- **Priority Level**: P0 (Critical), P1 (High), P2 (Medium), P3 (Low)
- **Category Tags**: @smoke, @regression, @ui, @backend
- **Allure Annotations**: Epic, Feature, Story, Severity
- **Requirements Traceability**: Links to requirements (REQ-SYS-XXX)
- **Detailed Docstrings**: Google style with Args, Returns, Raises

#### Example Test Structure

```python
@allure.epic("IWSVA System Verification")
@allure.feature("System Updates Page")
@allure.story("TC-SYS-001: Page Display and Navigation")
@allure.severity(allure.severity_level.CRITICAL)
@allure.testcase("TC-SYS-001-01", "Page Load Verification")
@pytest.mark.smoke
@pytest.mark.ui
@pytest.mark.P0
def test_page_load_and_title(self, driver, system_update_page):
    """
    TC-SYS-001 Test 1: Verify System Updates page loads with correct title.

    Preconditions:
    - User is logged in to IWSVA
    - User has access to System Updates page

    Test Steps:
    1. Navigate to System Updates page
    2. Verify page title contains 'System Update'
    3. Verify kernel information section is displayed

    Expected Results:
    - Page loads within 5 seconds
    - Page title is 'System Update'
    - Kernel information section is visible

    Args:
        driver: WebDriver instance
        system_update_page: System Update page object

    Raises:
        AssertionError: If any verification fails
    """
    # Test implementation...
```

---

## ğŸ“ Project Structure

```
selenium-tests/
â”œâ”€â”€ config/                          # Configuration management
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_config.py              # Centralized test configuration
â”‚   â””â”€â”€ logging_config.py           # Logging configuration
â”‚
â”œâ”€â”€ helpers/                         # Utility modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py                   # Enterprise logging system
â”‚   â”œâ”€â”€ debug_helper.py             # Debug artifact capture
â”‚   â”œâ”€â”€ ssh_helper.py               # SSH operations (Paramiko)
â”‚   â””â”€â”€ video_recorder.py           # Test execution recording
â”‚
â”œâ”€â”€ pages/                           # Page Object Model
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_page.py                # Base page class
â”‚   â”œâ”€â”€ login_page.py               # Login page
â”‚   â””â”€â”€ system_update_page.py       # System Updates page
â”‚
â”œâ”€â”€ verification/                    # Verification modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ backend_verification.py     # SSH-based backend checks
â”‚   â”œâ”€â”€ ui_verification.py          # UI-level verification
â”‚   â””â”€â”€ log_verification.py         # Log file verification
â”‚
â”œâ”€â”€ tests/                           # Test specifications
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                 # Pytest fixtures and hooks
â”‚   â””â”€â”€ test_system_updates_enterprise.py  # Enterprise test suite
â”‚
â”œâ”€â”€ fixtures/                        # Test data
â”‚   â”œâ”€â”€ test_data.json
â”‚   â””â”€â”€ credentials.json.example
â”‚
â”œâ”€â”€ reports/                         # Test reports (auto-generated)
â”‚   â”œâ”€â”€ allure-results/             # Allure raw data
â”‚   â”œâ”€â”€ allure-report/              # Allure HTML report
â”‚   â”œâ”€â”€ report.html                 # Pytest HTML report
â”‚   â””â”€â”€ report.json                 # JSON report for CI/CD
â”‚
â”œâ”€â”€ screenshots/                     # Screenshots (auto-generated)
â”‚   â”œâ”€â”€ TC-SYS-001_20250109_143025.png
â”‚   â””â”€â”€ TC-SYS-001_20250109_143025.html
â”‚
â”œâ”€â”€ logs/                            # Log files (auto-generated)
â”‚   â”œâ”€â”€ test_20250109.log           # Daily log file (rotated)
â”‚   â””â”€â”€ pytest.log                  # Pytest execution log
â”‚
â”œâ”€â”€ videos/                          # Test execution videos (optional)
â”‚
â”œâ”€â”€ .env.example                     # Environment variables template
â”œâ”€â”€ .gitignore                       # Git ignore rules
â”œâ”€â”€ pytest.ini                       # Pytest configuration
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # This file
```

---

## âš¡ Quick Start

### 1. Prerequisites

- **Python 3.8+** installed
- **Chrome/Firefox** browser installed
- **IWSVA server** accessible
- **SSH access** to IWSVA (for backend verification)

### 2. Installation

```bash
# Clone repository
cd selenium-tests

# Create virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your credentials and server details
```

### 3. Configuration

Create `.env` file with your configuration:

```bash
# Application Configuration
BASE_URL=https://10.206.201.9:8443
USERNAME=admin
PASSWORD=your_password

# SSH Configuration (for backend verification)
SSH_HOST=10.206.201.9
SSH_PORT=22
SSH_USERNAME=root
SSH_PASSWORD=your_ssh_password

# Browser Configuration
BROWSER=chrome
HEADLESS=false
BROWSER_WIDTH=1920
BROWSER_HEIGHT=1080

# Test Configuration
TARGET_KERNEL_VERSION=5.14.0-427.24.1.el9_4.x86_64
MAX_RETRIES=2
LOG_LEVEL=INFO

# Optional: Video Recording
ENABLE_VIDEO=false
```

### 4. Run Tests

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_system_updates_enterprise.py -v

# Run with specific markers
pytest -m smoke -v           # Smoke tests only
pytest -m "P0 and ui" -v     # P0 UI tests only

# Run with Allure report
pytest --alluredir=reports/allure-results
allure serve reports/allure-results
```

---

## ğŸ”§ Configuration

### pytest.ini

Key configuration options:

```ini
[pytest]
# Markers for test categorization
markers =
    smoke: Smoke tests for critical functionality
    regression: Regression tests for all features
    ui: UI-level tests
    backend: Backend verification tests
    P0: Priority 0 - Critical tests

# Automatic features
addopts =
    --verbose
    --html=reports/report.html
    --alluredir=reports/allure-results
    --reruns=2                    # Retry failed tests 2 times
    --reruns-delay=1              # Wait 1 second between retries

# Timeout
timeout = 300  # 5 minutes per test
```

### TestConfig

Centralized configuration in `config/test_config.py`:

```python
from config.test_config import TestConfig

# Access configuration
print(TestConfig.BASE_URL)
print(TestConfig.TARGET_KERNEL_VERSION)
print(TestConfig.SSH_CONFIG)

# Validate configuration
TestConfig.validate_config()
```

---

## ğŸƒ Running Tests

### Basic Execution

```bash
# Run all tests
pytest

# Run with verbose output
pytest -v

# Run specific test class
pytest tests/test_system_updates_enterprise.py::TestSystemUpdatesEnterprise

# Run specific test method
pytest tests/test_system_updates_enterprise.py::TestSystemUpdatesEnterprise::test_page_load_and_title
```

### Filter by Markers

```bash
# Run smoke tests
pytest -m smoke

# Run UI tests only
pytest -m ui

# Run P0 priority tests
pytest -m P0

# Combine markers
pytest -m "smoke and ui"
pytest -m "P0 or P1"
pytest -m "smoke and not backend"
```

### Parallel Execution

```bash
# Run tests in parallel (4 workers)
pytest -n 4

# Run tests in parallel (auto-detect CPU count)
pytest -n auto
```

### Custom Options

```bash
# Run in headless mode
HEADLESS=true pytest

# Use Firefox instead of Chrome
BROWSER=firefox pytest

# Increase log verbosity
LOG_LEVEL=DEBUG pytest

# Enable video recording
ENABLE_VIDEO=true pytest
```

---

## ğŸ“Š Test Reports

### 1. HTML Report (Pytest-HTML)

**Location**: `reports/report.html`

```bash
pytest --html=reports/report.html --self-contained-html
```

Open `reports/report.html` in browser to view:
- Test execution summary
- Pass/Fail statistics
- Test duration
- Captured logs and screenshots

### 2. Allure Report (Recommended)

**Location**: `reports/allure-report/`

```bash
# Generate Allure results
pytest --alluredir=reports/allure-results

# Generate and serve Allure report
allure serve reports/allure-results

# Generate static HTML report
allure generate reports/allure-results -o reports/allure-report --clean
```

Allure report includes:
- ğŸ“Š **Dashboards**: Overview, trends, categories
- ğŸ“ **Test cases**: Detailed steps, attachments, logs
- ğŸ“ˆ **Graphs**: Duration, severity, features
- ğŸ” **Traceability**: Requirements mapping
- ğŸ“¸ **Screenshots**: Embedded in test steps

### 3. JSON Report (CI/CD)

**Location**: `reports/report.json`

```bash
pytest --json-report --json-report-file=reports/report.json
```

Parseable JSON for CI/CD integration.

---

## ğŸ› Debugging

### Automatic Failure Capture

When a test fails, the framework automatically captures:

1. **Screenshot** (PNG)
2. **HTML source code** (HTML)
3. **Browser console logs** (LOG)
4. **Page state information** (JSON)

All artifacts are saved with timestamp in `screenshots/` directory.

### Manual Debugging

```python
from helpers.debug_helper import DebugHelper

# Capture screenshot manually
DebugHelper.capture_screenshot(driver, "my_checkpoint")

# Save page source
DebugHelper.save_page_source(driver, "error_state")

# Capture full failure artifacts
DebugHelper.capture_failure_artifacts(
    driver,
    "test_name",
    "TC-XXX-XX",
    exception
)
```

### Debug Context Manager

```python
from helpers.debug_helper import DebugContext

with DebugContext(driver, "login_flow", capture_screenshot=True) as debug:
    driver.find_element(By.ID, 'username').send_keys('admin')
    debug.checkpoint("Username entered")

    driver.find_element(By.ID, 'password').send_keys('password')
    debug.checkpoint("Password entered")

    driver.find_element(By.ID, 'submit').click()
    debug.checkpoint("Login submitted")
```

### Viewing Logs

```bash
# Real-time log monitoring
tail -f logs/test_$(date +%Y%m%d).log

# View Pytest log
cat logs/pytest.log

# View browser logs
cat screenshots/*_browser.log
```

---

## ğŸ“š Best Practices

### 1. Test Structure

âœ… **DO**: Follow AAA pattern (Arrange, Act, Assert)

```python
def test_example(self, driver):
    # Arrange
    TestLogger.log_step("Setup test data")

    # Act
    TestLogger.log_step("Perform action")

    # Assert
    TestLogger.log_step("Verify results")
```

### 2. Logging

âœ… **DO**: Log every important step

```python
TestLogger.log_step("Navigate to login page")
TestLogger.log_verification("Username", "admin", actual_username, True)
TestLogger.log_performance("Page load", duration, threshold=5.0)
```

### 3. Error Handling

âœ… **DO**: Use try-finally for cleanup

```python
try:
    self.switch_to_frame(driver, 'right')
    content = driver.find_element(By.TAG_NAME, 'body').text
finally:
    driver.switch_to.default_content()
```

### 4. Page Object Model

âœ… **DO**: Use page objects, not direct driver calls in tests

```python
# Good
system_update_page.navigate()
system_update_page.get_kernel_version()

# Bad
driver.find_element(By.ID, 'kernel_version').text
```

---

## ğŸ”„ CI/CD Integration

### GitHub Actions Example

```yaml
name: Selenium Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run tests
        run: |
          pytest tests/ -v --html=reports/report.html --alluredir=reports/allure-results
        env:
          BASE_URL: ${{ secrets.BASE_URL }}
          USERNAME: ${{ secrets.USERNAME }}
          PASSWORD: ${{ secrets.PASSWORD }}

      - name: Upload Allure results
        uses: actions/upload-artifact@v3
        with:
          name: allure-results
          path: reports/allure-results

      - name: Publish Allure report
        uses: simple-elf/allure-report-action@master
        if: always()
        with:
          allure_results: reports/allure-results
          allure_report: allure-report
```

---

## ğŸ“ Support

For questions or issues:
- Create an issue in the repository
- Contact: QA Automation Team
- Documentation: See `/docs` directory

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

**Built with â¤ï¸ by QA Automation Team**
